{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Realest POTUS Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project will be split into 2 parts (given the limitations of webscraping Twitter, I will postpone that part as an addition later on). For now, I'll use the data found on https://www.kaggle.com/austinreese/trump-tweets:\n",
    "* 1. *NLP* - using the dataset scrapped, I'll make a NLP model that will generate fake twits\n",
    "* 2. *Basic UI* - I'll make a python file that abstract the function from the NLP into a usable, more fun manner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"realdonaldtrump.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1698308935</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/169...</td>\n",
       "      <td>Be sure to tune in and watch Donald Trump on L...</td>\n",
       "      <td>2009-05-04 13:54:25</td>\n",
       "      <td>510</td>\n",
       "      <td>917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1701461182</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/170...</td>\n",
       "      <td>Donald Trump will be appearing on The View tom...</td>\n",
       "      <td>2009-05-04 20:00:10</td>\n",
       "      <td>34</td>\n",
       "      <td>267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1737479987</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/173...</td>\n",
       "      <td>Donald Trump reads Top Ten Financial Tips on L...</td>\n",
       "      <td>2009-05-08 08:38:08</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1741160716</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/174...</td>\n",
       "      <td>New Blog Post: Celebrity Apprentice Finale and...</td>\n",
       "      <td>2009-05-08 15:40:15</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1773561338</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/177...</td>\n",
       "      <td>\"My persona will never be that of a wallflower...</td>\n",
       "      <td>2009-05-12 09:07:28</td>\n",
       "      <td>1375</td>\n",
       "      <td>1945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               link  \\\n",
       "0  1698308935  https://twitter.com/realDonaldTrump/status/169...   \n",
       "1  1701461182  https://twitter.com/realDonaldTrump/status/170...   \n",
       "2  1737479987  https://twitter.com/realDonaldTrump/status/173...   \n",
       "3  1741160716  https://twitter.com/realDonaldTrump/status/174...   \n",
       "4  1773561338  https://twitter.com/realDonaldTrump/status/177...   \n",
       "\n",
       "                                             content                 date  \\\n",
       "0  Be sure to tune in and watch Donald Trump on L...  2009-05-04 13:54:25   \n",
       "1  Donald Trump will be appearing on The View tom...  2009-05-04 20:00:10   \n",
       "2  Donald Trump reads Top Ten Financial Tips on L...  2009-05-08 08:38:08   \n",
       "3  New Blog Post: Celebrity Apprentice Finale and...  2009-05-08 15:40:15   \n",
       "4  \"My persona will never be that of a wallflower...  2009-05-12 09:07:28   \n",
       "\n",
       "   retweets  favorites mentions hashtags  \n",
       "0       510        917      NaN      NaN  \n",
       "1        34        267      NaN      NaN  \n",
       "2        13         19      NaN      NaN  \n",
       "3        11         26      NaN      NaN  \n",
       "4      1375       1945      NaN      NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try taking tweet from 2015 onwards since that's more of the Trump we know (and 'love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43352 entries, 0 to 43351\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         43352 non-null  int64 \n",
      " 1   link       43352 non-null  object\n",
      " 2   content    43352 non-null  object\n",
      " 3   date       43352 non-null  object\n",
      " 4   retweets   43352 non-null  int64 \n",
      " 5   favorites  43352 non-null  int64 \n",
      " 6   mentions   20386 non-null  object\n",
      " 7   hashtags   5583 non-null   object\n",
      "dtypes: int64(3), object(5)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to change dates to the right format\n",
    "tweets['date'] = pd.to_datetime(tweets['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43352 entries, 0 to 43351\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   id         43352 non-null  int64         \n",
      " 1   link       43352 non-null  object        \n",
      " 2   content    43352 non-null  object        \n",
      " 3   date       43352 non-null  datetime64[ns]\n",
      " 4   retweets   43352 non-null  int64         \n",
      " 5   favorites  43352 non-null  int64         \n",
      " 6   mentions   20386 non-null  object        \n",
      " 7   hashtags   5583 non-null   object        \n",
      "dtypes: datetime64[ns](1), int64(3), object(4)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "#voila\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "president_twts = tweets[tweets['date'].dt.year.between(2015,2020)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antho\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#renmoving hyperlinks in the tweets\n",
    "president_twts['content'] = president_twts['content'].str.replace('http\\S+|www.\\S+|pic.twitter\\S+', '', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" @ JustSoldCom: @ realDonaldTrump SO HAPPY # CelebrityApprentice is back on! Thanks Donald for keeping it real!\"'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "president_twts['content'].iloc[150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" @ about_life: @ realDonaldTrump I Luv America 4 Trump 2016 - Let’s Make America Great Again! - iluvamerica4trump2016@gmail.com\"\n",
      "\" @ about_life: @ realDonaldTrump Silent Warriors 4 Trump 2016 - Vets! Let’s Take America Back! - silentwarriors4trump2016@gmail.com\"\n",
      "\" @ about_life: Politician Have Been Running(Ruining) America For Years; Lets Change That Now -Trump 2016 - iluvamerica4trump2016@gmail.com\"\n",
      "Veterans, please call 855- VETS- 352 or email address veterans@donaldtrump.com to share your stories about the need to reform the VA.\n",
      "...completely avoided if you buy from a non-Tariffed Country, or you buy the product inside the USA (the best idea). That’s Zero Tariffs. Many Tariffed companies will be leaving China for Vietnam and other such countries in Asia. That’s why China wants to make a deal so badly!...\n",
      "....companies to come to the USA and to get companies that have left us for other lands to COME BACK HOME. We stupidly lost 30% of our auto business to Mexico. If the Tariffs went on at the higher level, they would all come back, and fast. But very happy with the deal I made,...\n",
      "....competitive disadvantage. We should be leading, not following!\n"
     ]
    }
   ],
   "source": [
    "i = 0 \n",
    "for row in president_twts['content']:    \n",
    "    if '.com' in row and i < 20:\n",
    "        print(row)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19465         \"@flicka__: @ realDonaldTrump for president\"\n",
       "19466    The Mar-a-Lago Club was amazing tonight. Every...\n",
       "19467    \" @ archangeljf12: ;, @ realDonaldTrump for Pr...\n",
       "19468    \"@TalentlessCook: @ realDonaldTrump You're onl...\n",
       "19469    \" @ yankeejayman: @ realDonaldTrump @flicka__ ...\n",
       "                               ...                        \n",
       "43347    Joe Biden was a TOTAL FAILURE in Government. H...\n",
       "43348    Will be interviewed on @ seanhannity tonight a...\n",
       "43349                                                     \n",
       "43350                                                     \n",
       "43351                                                     \n",
       "Name: content, Length: 23887, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "president_twts['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = '\\n '.join([row for row in president_twts['content']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"@flicka__: @ realDonaldTrump for president\"\\n The Mar-a-Lago Club was amazing tonight. Everybody was there, the biggest and the hottest. Palm Beach is so lucky to have best club in world\\n \" @ archangeljf12: ;, @ realDonaldTrump for President of the United States! @ SenTedCruz Vice President ;A # Win'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The first model will be quite basic and will be character based\"\"\"\n",
    "vocab=sorted(set(text_data))\n",
    "char_to_ind={char:ind for ind,char in enumerate(vocab)} #c_t_i['H'] = 33\n",
    "ind_to_char=np.array(vocab) #i_t_c[33] = 'H'\n",
    "encoded_text=np.array([char_to_ind[c] for c in text_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '«',\n",
       " '´',\n",
       " '»',\n",
       " '½',\n",
       " 'É',\n",
       " 'á',\n",
       " 'â',\n",
       " 'è',\n",
       " 'é',\n",
       " 'í',\n",
       " 'ï',\n",
       " 'ñ',\n",
       " 'ò',\n",
       " 'ó',\n",
       " 'ô',\n",
       " 'ö',\n",
       " 'ø',\n",
       " 'ú',\n",
       " 'ğ',\n",
       " 'ı',\n",
       " 'ĺ',\n",
       " 'ō',\n",
       " 'א',\n",
       " 'ב',\n",
       " 'ג',\n",
       " 'ד',\n",
       " 'ה',\n",
       " 'ו',\n",
       " 'ז',\n",
       " 'ח',\n",
       " 'ט',\n",
       " 'י',\n",
       " 'ך',\n",
       " 'כ',\n",
       " 'ל',\n",
       " 'ם',\n",
       " 'מ',\n",
       " 'ן',\n",
       " 'נ',\n",
       " 'ס',\n",
       " 'ע',\n",
       " 'צ',\n",
       " 'ק',\n",
       " 'ר',\n",
       " 'ש',\n",
       " 'ת',\n",
       " '،',\n",
       " 'ء',\n",
       " 'آ',\n",
       " 'أ',\n",
       " 'ؤ',\n",
       " 'ا',\n",
       " 'ب',\n",
       " 'ة',\n",
       " 'ت',\n",
       " 'ج',\n",
       " 'ح',\n",
       " 'خ',\n",
       " 'د',\n",
       " 'ذ',\n",
       " 'ر',\n",
       " 'ز',\n",
       " 'س',\n",
       " 'ش',\n",
       " 'ص',\n",
       " 'ض',\n",
       " 'ط',\n",
       " 'ظ',\n",
       " 'ع',\n",
       " 'ف',\n",
       " 'ق',\n",
       " 'ك',\n",
       " 'ل',\n",
       " 'م',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ي',\n",
       " 'ً',\n",
       " 'چ',\n",
       " 'ژ',\n",
       " 'ک',\n",
       " 'گ',\n",
       " 'ی',\n",
       " '۰',\n",
       " '۴',\n",
       " 'ँ',\n",
       " 'ं',\n",
       " 'अ',\n",
       " 'आ',\n",
       " 'इ',\n",
       " 'उ',\n",
       " 'ए',\n",
       " 'औ',\n",
       " 'क',\n",
       " 'ख',\n",
       " 'ग',\n",
       " 'घ',\n",
       " 'च',\n",
       " 'छ',\n",
       " 'ज',\n",
       " 'झ',\n",
       " 'ट',\n",
       " 'ठ',\n",
       " 'ड',\n",
       " 'त',\n",
       " 'थ',\n",
       " 'द',\n",
       " 'ध',\n",
       " 'न',\n",
       " 'प',\n",
       " 'ब',\n",
       " 'भ',\n",
       " 'म',\n",
       " 'य',\n",
       " 'र',\n",
       " 'ल',\n",
       " 'व',\n",
       " 'श',\n",
       " 'ष',\n",
       " 'स',\n",
       " 'ह',\n",
       " '़',\n",
       " 'ा',\n",
       " 'ि',\n",
       " 'ी',\n",
       " 'ु',\n",
       " 'ू',\n",
       " 'े',\n",
       " 'ै',\n",
       " 'ो',\n",
       " '्',\n",
       " '।',\n",
       " 'ễ',\n",
       " '\\u200c',\n",
       " '–',\n",
       " '—',\n",
       " '―',\n",
       " '‘',\n",
       " '’',\n",
       " '“',\n",
       " '”',\n",
       " '•',\n",
       " '…',\n",
       " '′',\n",
       " '●',\n",
       " '☆',\n",
       " '☉',\n",
       " '☞',\n",
       " '♡',\n",
       " '➜',\n",
       " '《',\n",
       " 'ー',\n",
       " '\\U0010fc00']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after encoding text \n",
    "seq_len = 125 #length of a batch\n",
    "char_dataset=tensorflow.data.Dataset.from_tensor_slices(encoded_text)\n",
    "#creates special tf dataset object\n",
    "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)\n",
    "#creates batches of sequences \n",
    "\n",
    "def create_seq_targets(seq):\n",
    "        \"\"\"this pair is what the NLP model will try predict (the last letter)\"\"\"\n",
    "        input_txt = seq[:-1] #Hello my nam\n",
    "        target_txt = seq[1:] #ello my name\n",
    "        return input_txt, target_txt \n",
    "dataset = sequences.map(create_seq_targets)\n",
    "\n",
    "batch_size = 128\n",
    "buffer_size = 100000 #shuffle by batch to prevent memory issue\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 203 steps\n",
      "Epoch 1/30\n",
      "203/203 [==============================] - 21s 101ms/step - loss: 3.0725\n",
      "Epoch 2/30\n",
      "203/203 [==============================] - 18s 89ms/step - loss: 2.2224\n",
      "Epoch 3/30\n",
      "203/203 [==============================] - 19s 92ms/step - loss: 1.8343\n",
      "Epoch 4/30\n",
      "203/203 [==============================] - 18s 90ms/step - loss: 1.5914\n",
      "Epoch 5/30\n",
      "203/203 [==============================] - 18s 91ms/step - loss: 1.4549\n",
      "Epoch 6/30\n",
      "203/203 [==============================] - 18s 89ms/step - loss: 1.3699\n",
      "Epoch 7/30\n",
      "203/203 [==============================] - 18s 88ms/step - loss: 1.3114\n",
      "Epoch 8/30\n",
      "203/203 [==============================] - 18s 89ms/step - loss: 1.2651\n",
      "Epoch 9/30\n",
      "203/203 [==============================] - 19s 92ms/step - loss: 1.2279\n",
      "Epoch 10/30\n",
      "203/203 [==============================] - 18s 90ms/step - loss: 1.1961\n",
      "Epoch 11/30\n",
      "203/203 [==============================] - 19s 93ms/step - loss: 1.1664\n",
      "Epoch 12/30\n",
      "203/203 [==============================] - 19s 91ms/step - loss: 1.1409\n",
      "Epoch 13/30\n",
      "203/203 [==============================] - 18s 91ms/step - loss: 1.1166\n",
      "Epoch 14/30\n",
      "203/203 [==============================] - 18s 88ms/step - loss: 1.0938\n",
      "Epoch 15/30\n",
      "203/203 [==============================] - 19s 92ms/step - loss: 1.0723\n",
      "Epoch 16/30\n",
      "203/203 [==============================] - 18s 91ms/step - loss: 1.0512\n",
      "Epoch 17/30\n",
      "203/203 [==============================] - 19s 92ms/step - loss: 1.0325\n",
      "Epoch 18/30\n",
      "203/203 [==============================] - 18s 90ms/step - loss: 1.0143\n",
      "Epoch 19/30\n",
      "203/203 [==============================] - 18s 91ms/step - loss: 0.9969\n",
      "Epoch 20/30\n",
      "203/203 [==============================] - 19s 92ms/step - loss: 0.9799\n",
      "Epoch 21/30\n",
      "203/203 [==============================] - 19s 93ms/step - loss: 0.9651\n",
      "Epoch 22/30\n",
      "203/203 [==============================] - 19s 92ms/step - loss: 0.9508\n",
      "Epoch 23/30\n",
      "203/203 [==============================] - 19s 92ms/step - loss: 0.9371\n",
      "Epoch 24/30\n",
      "203/203 [==============================] - 26s 126ms/step - loss: 0.9249\n",
      "Epoch 25/30\n",
      "203/203 [==============================] - 19s 93ms/step - loss: 0.9134\n",
      "Epoch 26/30\n",
      "203/203 [==============================] - 19s 93ms/step - loss: 0.9036\n",
      "Epoch 27/30\n",
      "203/203 [==============================] - 19s 94ms/step - loss: 0.8945\n",
      "Epoch 28/30\n",
      "203/203 [==============================] - 19s 91ms/step - loss: 0.8861\n",
      "Epoch 29/30\n",
      "203/203 [==============================] - 18s 90ms/step - loss: 0.8792\n",
      "Epoch 30/30\n",
      "203/203 [==============================] - 19s 93ms/step - loss: 0.8720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2448e879488>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size=len(vocab)\n",
    "embed_dim=64\n",
    "rnn_neurons=1026\n",
    "\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "def sparse_cat_loss(y_true,y_pred):\n",
    "        return sparse_categorical_crossentropy(y_true,y_pred,from_logits=True)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "\n",
    "def create_model(vocab_size,embed_dim,rnn_neurons,batch_size):\n",
    "        model=Sequential()\n",
    "        model.add(Embedding(vocab_size,embed_dim,\n",
    "                                             batch_input_shape=[batch_size, None]))\n",
    "        model.add(GRU(rnn_neurons,return_sequences=True,\n",
    "                                   stateful=True, recurrent_initializer='glorot_uniform'))\n",
    "        #stateful = last state for each esample in batch will be used as initial state for following batch\n",
    "        \n",
    "        model.add(Dense(vocab_size))\n",
    "        model.compile(optimizer='adam', loss=sparse_cat_loss)\n",
    "        return model\n",
    "\n",
    "model=create_model(vocab_size=vocab_size,\n",
    "                                     embed_dim=embed_dim,\n",
    "                                     rnn_neurons=rnn_neurons,\n",
    "                                     batch_size=batch_size)\n",
    "\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (128, None, 64)           15872     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (128, None, 1026)         3361176   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (128, None, 248)          254696    \n",
      "=================================================================\n",
      "Total params: 3,631,744\n",
      "Trainable params: 3,631,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('D:/data_science/GIT/Trump-Twit-Generator/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow.keras.models.save_model(model, 'D:/data_science/GIT/Trump-Twit-Generator/my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after training NLP model\n",
    "def generate_text(model,start_seed,gen_size=500,temp=1.0):\n",
    "    input_eval = [char_to_ind[s] for s in start_seed]  #transforming string back into index location\n",
    "    input_eval = tensorflow.expand_dims(input_eval,0) #expand to fit shape of model\n",
    "    text_generated = [] \n",
    "    model.reset_states()\n",
    "\n",
    "    for i in range(gen_size):\n",
    "        predictions=model(input_eval)\n",
    "        predictions=tensorflow.squeeze(predictions,0) #undo the expand dims\n",
    "        predictions=predictions/temp #affect prob distribution based on temp\n",
    "        predicted_id=tensorflow.random.categorical(predictions,num_samples=1)[-1,0].numpy()\n",
    "        #grabbing the index and converted into np to get single digit\n",
    "        input_eval=tensorflow.expand_dims([predicted_id],0)\n",
    "        text_generated.append(ind_to_char[predicted_id])\n",
    "    return(start_seed+\"\".join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(vocab_size,embed_dim,rnn_neurons,batch_size = 1)\n",
    "model.load_weights('my_model.h5')\n",
    "model.build(tensorflow.TensorShape([1,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 64)             15872     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1026)           3361176   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 248)            254696    \n",
      "=================================================================\n",
      "Total params: 3,631,744\n",
      "Trainable params: 3,631,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hillary for Pennsylvania! Looks like Monda on # Trump16 that's only one won—much to fight back!!\"\n",
      " \" @ LutorontFounHe: @ realDonaldT\n",
      "Hillary facts go over the loo: \n",
      " THANK YOU DOWITH in over 500,000 for the Washington Poll - out to @ realDonaldTrump and be the secu\n",
      "Hillary Clinton 49% # Trump2016 \n",
      " \"@Bdendenk978: @ realDonaldTrump @ bluestar vetor now!\n",
      " Legal immigrants tapped. Remember, they co\n",
      "Hillary Clinton cannot Trump on Military, our Vets and the 2018 @ TeamCavuto @toulp keep down! # MakeAmericaGreatAgain\"\n",
      " \" @ DhuedRV\n",
      "Hillary Clinton is a good thing, not a bad thing. The unemployment rate for life, to be a fine jumpt rominand the illegal immigratio\n",
      "Hillary’s Veterans! Love tell Trump he's in rare. My great honor!\n",
      " The great people of Puerto Rico as possible, I never seen bowh in\n",
      "Hillary Clinton ‘smart & very bright Investigation Hear), think Her...\n",
      " ...it was a little conflict along Mini Mike credibility much\n",
      "Hillary\" on the same things. Save of deals has put a folult we can never beat let us down. But I'm to lift. I am very partical ssitt\n",
      "Hillary Clinton he received a case. Great timine. THANK YOU @ SlamaNBC\"\n",
      " That Mude CDN Walte to If anything!\n",
      " Come on Friday night J\n",
      "Hillary scheck. It was supposed to, but many people verue. She lost to do.\"\n",
      " \" @ SpeakerRoe: Trump Leads to Covey.”\n",
      " When will Secre\n",
      "Hillary -- nothing told done on the board where just now admitted system illight. In the last year of the crowds, not wait the Cross\n",
      "Hillary's Voter I was always to know what's going to be very happy stent their funding for political required @ Jim_Jordon, Nov. Lou\n",
      "Hillary Clinton is actually dropped out of the race? She look away! Standing up the fact that we stopped social media suscess probab\n",
      "Hillary Clinton Election victory in Washington, D.C., but since the Election, hy entourible Carson and The Apprentice\"\n",
      " \" @ joenchin\n",
      "Hillary Clinton is much more to be faith ng to play by the Stabulous Day and Testing in 81 years! They are trying to distand a GREAT\n",
      "Hillary of State of to163 together (and simple), not look ratings and are out of conversations at the G-7 instead of people! SOIR\" H\n",
      "Hillary's book. Will be fun!\n",
      " Congress fully trashy. Shouldn’t be @ senator will be making a fortune. Perhaps Unvestigation I'm voti\n",
      "Hillary Clinton is totally rigged and compromise in his phony voice. It was just paid for by Crooked Hillary. Sends the same very ba\n",
      "Hillary Clinton Founderning with my conversation. # Am # Trump2016\" Thanks.\n",
      " \" @ TheMikeTurts @ realDonaldTrump @ NBOWever @ realDon\n",
      "Hillary's Star.\" True!\n",
      " \" @ JNorrick423: Joe Billy Gradlaidercis is a president, rather policies are must be at @ HogaDinee Of the B\n"
     ]
    }
   ],
   "source": [
    "#what does Donald think about Hillary\n",
    "for i in range(0,20):\n",
    "    print(generate_text(model, \"Hillary\", gen_size=125))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama Coant Down is dead. San to work for our stand. Congressman Megyn.\n",
      " \" @ bigor03: Peloni 60% @ slunting @ realDonaldTrump Caug\n",
      "Obama from the Great State of Independence. The reason tour own help fronting the truth. Mike will ifly using all of you!\n",
      " Great j\n",
      "Obama of ABC, SCMPAST PITI- MARCED Decision was hitchungred. I only seeks a \n",
      " \" @ ilwing123: I'm very talented reportership was a \n",
      "Obama 'House's showed really things what is going on in this Whole Widdwhere muden trying to do, and we should try to get out over\n",
      "Obama/and KNY toarch sutcess of Leftalicts yet, but spould go up deficies. # Trump2016\n",
      " # VoteTrumpMI, # Trump2016\n",
      " THANK YOU NORT\n",
      "Obama voters have to use media outlets, no sources, there was NO COLLUSION and NATO people. Everybody and they refuse to see that \n",
      "Obama last year.\n",
      " Bad lia, it was Lushed 7.7% in Washington Antressive caring out the now disgraceful showed a great Congressiona \n",
      "Obama jobs on now management doesn't she doesn't want to go down to warmive butthe FBI lovers with a 3Bound higher. Stupidly destr\n",
      "Obama, Crooked Hillary Clinton announced to aster so quick keep is going up fast enough. Make America Great Again!\n",
      " . @ TheCingled\n",
      "Obama/Say, getting this place to make their @ Macys concerning Israel’s @SAP poll! @ megynkelly\n",
      " So true, a total loser! …\n",
      " The Wh\n",
      "Obama700NCRUCHIMON THANK GERMLESS EVERYTHING BIG FOR THREES HEPONS, NO ATTIONY, Fake Dapas, THANK YOU!\n",
      " See @ nbc and smyones  \"\n",
      " \n",
      "Obama/and Reform, Boring! …\n",
      " Statement and Schumer and Open Borders and so many of a scandal here!\n",
      " I beat Hillary eir part for yo\n",
      "Obama handshould be boing with little respect for President! They say no wonder you would be a difference.\n",
      " \" @ Morning_Joe: The R\n",
      "Obama groups and most powerful mind. Their right deals have been fully in!\n",
      " . @ SenTedCruz end! # Trump2016\n",
      " \" @ news: There are n\n",
      "Obamation has done more than any other State, before it is too found November probe in the polls, tabe at the Southern Border. Whe\n",
      "Obama and inst all the day to run for president\"  …\n",
      " CRIME IT READ THE RIGGED INTRESCINES \" THANK YOU!\"\n",
      " \" @ mishongjea19:U. # Tru\n",
      "Obama/'K @ nycojohi9 @ realDonaldTrump @ GOP no chance, I'm reifing our indfbed so that you are using you in many capability in ou\n",
      "Obama for your brave!\n",
      " The failing @ NRO National poll done done. A great book!\n",
      " Great article from the Young of the Appeally Hoax\n",
      "Obama/about @ realDonaldTrump 4 TRUTH where would have accomplet is actually going to have Build From vadadies. We will soon signe\n",
      "Obama facility, is such a best ever, especially weeks to keep common corrupt at a terrible job on Border Security.Their Pid Turkey\n"
     ]
    }
   ],
   "source": [
    "#what does Donald think about Obama\n",
    "for i in range(0,20):\n",
    "    print(generate_text(model, \"Obama\", gen_size=125))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coronaVATrump just slong me and only Steve Hife” Transcript of the popular India shipping immediately!\n",
      " Sad to lose the drive of Pr\n",
      "coronaVIG: Donald Trump: April in @ TrumpDoral being better reporting in the hopents people like stephing up the House to be on the\n",
      "coronaVirus Moting over” but they don’t know what we have a n this mon. We should also dismusted for the U.S., doing an insult to a\n",
      "corona dropped authorities to bobbing air lowest the totally biased in talks to be moving point. He didn't they real change.\n",
      " Entre\n",
      "coronaVirus: \n",
      " # SmallBeNnieThow trought that gives upeace and what most of the problem. Because it is far greater than any other c\n",
      "coronaVY vood vicious and totally good to determine to both, love that Spying on Bronnjust delivered live nomination.\n",
      " Entry rebuon\n",
      "coronastructuary as a Rocky two great Space. Here idease FBI Director in the poll done, big trial!\n",
      " The Fake News Media hates to sa\n",
      "coronavers. Prevaile must stop what @ realDonaldTrump does best. Amond make it all wrong it to MAKE AMERICA GREAT AGAIN VA winners,\n",
      "coronaVirus Tuesday- it as well today. I will be working & involved. Auto who do they will win!\n",
      " So sick and dead and two people of\n",
      "coronaVerser: Entrepreneurs: Kevin Chain @ Hean. You can do it!\"\n",
      " \" @ MROurt: No WAY join the Maine! …\n",
      " Amazing showed a long time,\n",
      "coronast run rooms!\"\n",
      " \" @ TomKimelyn: @ ApprenticeNBC @ realDonaldTrump @ Wexitate In the first of $'s do with the victims and toda\n",
      "coronaVIrse to stay! …\n",
      " Thank you Georgia! Should heard @ realDonaldTrump in very crowded, talks and totally probably what my peopl\n",
      "coronaVITUSA! Can't watch an Senator Country. Biggest politicians who weety?\n",
      " Great reviews and an emn late great representatives, \n",
      "coronaVirus Taskawe celeb people showned.\n",
      " .....Could “Trump” ‘I WILL BE AGAIN!\n",
      " The Two nice comment and they wouldn’t even called\n",
      "coronaVirus speech in State and Sec. Go golf and a strong pump stories. Hundreds of delegates.\n",
      " \n",
      " She did obstruct, while they don’\n",
      "coronaVirus statement from coming to the U.S. Steel?\n",
      " School, and many other planssional support! …\n",
      " School join behause we need Tr\n",
      "coronaVirus ECONBC MOSE INTERTIC with Russia/Conssitt. Capitalization.\n",
      " Remember Secret Sit, and Senator Schumer and Bib Barlden Co\n",
      "coronaVIrue\"\n",
      " \" @ tirgur_iends as large numbers of Federal Gurdemery Schumer is not suggeating a Sy-Is a failing organization to do\n",
      "coronaVorud”  … via @ tretionbe Carson 3 years and good (GREAT), & get out of time!\n",
      " GREAT NEWS OF THE PROSIS in Beire Farmiake whe\n",
      "coronaVIONBIN THE RROW\"\n",
      " Via @Newsmax_Media: \"Trump Hates for Mini Mike Bloomberg. If you think...\n",
      " ...Tames add by done & we will \n"
     ]
    }
   ],
   "source": [
    "#what does Donald think about Coronavirus\n",
    "for i in range(0,20):\n",
    "    print(generate_text(model, \"corona\", gen_size=125))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Making a useable UI on Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. make a function that allows user to input a word\n",
    "#### 2. randomise where the input word will be in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
